# ğŸ§½ Sponge Crawler

A powerful, user-friendly web crawler and document downloader that extracts content and documents from websites while respecting ethical crawling practices.

![Sponge Crawler Interface](https://img.shields.io/badge/Interface-Web%20%26%20CLI-blue)
![Node.js](https://img.shields.io/badge/Node.js-16+-green)
![License](https://img.shields.io/badge/License-MIT-blue)

## âœ¨ Features

### ğŸ¯ **Smart Crawling**
- **Intelligent Page Discovery** - Analyzes sitemaps and pagination patterns
- **Pre-Crawl Estimation** - Shows expected page count with confidence scoring
- **Configurable Depth Control** - Set crawling depth (1-10 levels)
- **Domain Filtering** - Stay within target domains or explore freely
- **Real-Time Progress** - Live monitoring with detailed statistics

### ğŸ“„ **Content Extraction**
- **Multiple Content Types** - HTML, Markdown, Plain Text page content
- **Document Detection** - Automatically finds PDFs, DOCs, images, and more
- **Flat File Structure** - Organized downloads in user-friendly format
- **On-Demand Downloads** - Files downloaded when requested (ZIP format)
- **Smart Status Display** - Clear "Available" status for all discoverable content

### ğŸŒ **Modern Web Interface**
- **Single-Page Layout** - Everything visible without scrolling
- **Compact Design** - Streamlined, focused user experience
- **Mobile Responsive** - Works on desktop, tablet, and mobile
- **Enhanced Tooltips** - Detailed explanations for all features
- **Bulk Downloads** - Download all found documents as ZIP

### ğŸ”§ **Advanced Options**
- **File Type Filtering** - Select specific document types to download
- **Rate Limiting** - Configurable delays to be respectful to target sites
- **Robust Error Handling** - Graceful timeouts and recovery
- **Export Formats** - JSON metadata, human-readable reports

## ğŸš€ Quick Start

### Prerequisites
- Node.js 16+ 
- npm or yarn

### Installation
```bash
# Clone the repository
git clone https://github.com/Jan-H2M/sponge.git
cd sponge

# Install dependencies
npm install

# Start the web interface
npm start
```

The web interface will be available at **http://127.0.0.1:3000**

### Basic Usage

1. **Enter Website URL** - Input the target website URL
2. **Configure Options** - Set crawl depth and select content types
3. **View Page Estimation** - See predicted page count and analysis
4. **Start Crawling** - Watch real-time progress
5. **Download Results** - Get all found documents as ZIP file

## ğŸ› ï¸ Usage

### Web Interface (Recommended)
The easiest way to use Sponge Crawler is through the streamlined web interface:

```bash
npm start
# Open http://127.0.0.1:3000 in your browser
```

**Features:**
- Compact single-page layout
- Pre-crawl page estimation with confidence scoring
- Real-time progress monitoring
- Bulk ZIP downloads
- Enhanced status descriptions

### Command Line Interface
```bash
# Basic crawl
npx sponge crawl https://example.com

# Advanced crawl with options
npx sponge crawl https://example.com -d 3 -o ./downloads --max-pages 500 --delay 2000

# Create configuration file
npm run config:init

# Test crawler setup
npx sponge test https://example.com
```

### Programmatic Usage
```javascript
const { SpongeCrawler } = require('./src/crawler/SpongeCrawler');

const config = {
    startUrl: 'https://example.com',
    maxDepth: 3,
    maxPages: 100,
    outputDir: './downloads',
    allowedFileTypes: ['pdf', 'doc', 'docx'],
    respectRobotsTxt: false  // Optional: ignore robots.txt
};

const crawler = new SpongeCrawler(config);
await crawler.crawl();
```

## ğŸ“ Configuration

### Content Types
Select from various content and document types:

**Page Content:**
- ğŸ“„ HTML pages
- ğŸ“ Markdown conversion  
- ğŸ“‹ Plain text extraction

**Documents:**
- ğŸ“ PDFs, Word docs, Excel files
- ğŸ–¼ï¸ Images (PNG, JPG, GIF, etc.)
- ğŸ“¦ Archives (ZIP, RAR, 7Z)
- ğŸµ Audio files (MP3, WAV, etc.)
- ğŸ¬ Video files (MP4, AVI, etc.)

### Crawl Settings
- **Max Depth**: How deep to crawl (1-10 levels)
- **Max Pages**: Maximum pages to visit (prevents runaway crawls)
- **Stay on Domain**: Limit crawling to the starting domain
- **Output Directory**: Where to save downloaded content

## ğŸ¯ Key Features Explained

### **Page Estimation**
Before crawling, Sponge analyzes the target website to:
- ğŸ“Š Estimate total crawlable pages
- ğŸ”— Detect pagination patterns
- ğŸ—ºï¸ Find and analyze sitemaps
- â­ Provide confidence ratings (HIGH/MEDIUM/LOW)

Example estimation results:
```
ğŸ“Š Page Estimation Results
Confidence: HIGH â­â­â­

Estimated Total: 2,267 pages
Pages Discovered: 0 (sitemap-based)
Pagination: No
Sitemap: Yes âœ…
```

### **Smart Document Detection**
The crawler intelligently identifies downloadable content:
- ğŸ” Scans page links for document URLs
- ğŸ“‹ Checks file extensions and MIME types
- ğŸ¯ Filters based on your selected file types
- ğŸ“ˆ Shows real-time discovery progress

### **On-Demand Downloads**
Efficient download system:
- ğŸ“ Documents detected and cataloged during crawling
- âš¡ Actual downloads happen when you request the ZIP
- ğŸ’¾ Saves bandwidth and storage during exploration
- ğŸ”„ Fresh downloads ensure latest versions

## ğŸ“Š Understanding Results

### Status Indicators
- **Available** ğŸŸ¢ - Document found and ready for download
- **Downloading** ğŸ”µ - Currently being fetched
- **Completed** âœ… - Successfully downloaded
- **Failed** âŒ - Download encountered an error

### Confidence Levels
- **HIGH** â­â­â­ - Based on sitemap analysis (very reliable)
- **MEDIUM** â­â­ - Based on link analysis (moderately reliable)  
- **LOW** â­ - Limited data available (less reliable)

## ğŸ›¡ï¸ Recent Improvements

### v2.0 Features
- âœ… **Fixed Status Display** - Documents now show "Available" instead of confusing "Pending"
- âœ… **Streamlined Interface** - Compact single-page layout without scrolling
- âœ… **Enhanced Page Estimation** - Detailed descriptions and confidence tooltips
- âœ… **Improved Error Handling** - Graceful timeouts and fallback responses
- âœ… **Robots.txt Handling** - Option removed (always ignored for better crawling)

## ğŸ”§ Development

### Project Structure
```
sponge/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler/          # Core crawling logic
â”‚   â”‚   â”œâ”€â”€ SpongeCrawler.js
â”‚   â”‚   â”œâ”€â”€ UrlQueue.js
â”‚   â”‚   â”œâ”€â”€ PageEstimator.js
â”‚   â”‚   â””â”€â”€ RobotsChecker.js
â”‚   â”œâ”€â”€ downloader/       # Document download handling
â”‚   â”‚   â””â”€â”€ DocumentDownloader.js
â”‚   â”œâ”€â”€ auth/            # Authentication support
â”‚   â”‚   â””â”€â”€ AuthManager.js
â”‚   â”œâ”€â”€ utils/           # Utility functions
â”‚   â”‚   â”œâ”€â”€ FilterManager.js
â”‚   â”‚   â””â”€â”€ Logger.js
â”‚   â”œâ”€â”€ export/          # Result export formats
â”‚   â”‚   â””â”€â”€ ExportManager.js
â”‚   â””â”€â”€ web-server.js    # Express server
â”œâ”€â”€ bin/sponge.js        # CLI entry point
â”œâ”€â”€ index.html          # Web interface
â”œâ”€â”€ app.js              # Frontend JavaScript
â”œâ”€â”€ styles.css          # Interface styling
â””â”€â”€ CLAUDE.md           # Technical documentation
```

### Available Scripts
```bash
npm start              # Start web interface
npm run web            # Alternative web interface command
npm run cli            # Run CLI directly
npm run dev            # Development with auto-reload
npm test               # Run tests
npm run lint           # Lint code
npm run build          # Build distribution package
```

### Server Management
```bash
npm run web:start      # Start server in background
npm run web:stop       # Stop background server
npm run web:restart    # Restart server
npm run web:logs       # View server logs
```
